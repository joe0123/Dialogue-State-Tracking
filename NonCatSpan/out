nohup: ignoring input
Not matched: ../datasets/data/train/dialogues_096.json | SSNG0201.json | hotel-name | hobsons house
Not matched: ../datasets/data/train/dialogues_097.json | MUL0152.json | taxi-departure | a and b guest house
Not matched: ../datasets/data/train/dialogues_097.json | MUL0152.json | hotel-name | a and b guest house
Not matched: ../datasets/data/train/dialogues_105.json | PMUL3484.json | taxi-departure | aylesbray lodge guest house
Not matched: ../datasets/data/train/dialogues_105.json | SNG01373.json | taxi-arriveby | one o'clock p.m
Not matched: ../datasets/data/train/dialogues_106.json | PMUL1303.json | bus-departure | cambridge
Not matched: ../datasets/data/train/dialogues_107.json | SNG01631.json | taxi-arriveby | ten o'clock a.m
Not matched: ../datasets/data/train/dialogues_108.json | WOZ20245.json | restaurant-name | pizza hut cherry hinton
Not matched: ../datasets/data/train/dialogues_108.json | SSNG0367.json | hotel-name | a and b guest house
Not matched: ../datasets/data/train/dialogues_112.json | SSNG0338.json | hotel-name | gonville hotel
Not matched: ../datasets/data/train/dialogues_115.json | SNG01929.json | taxi-arriveby | three forty five p.m
Not matched: ../datasets/data/train/dialogues_122.json | PMUL0192.json | hotel-name | rosas bed and breakfast
Not matched: ../datasets/data/train/dialogues_124.json | MUL1356.json | taxi-departure | university arms hotel
Not matched: ../datasets/data/train/dialogues_124.json | MUL1356.json | hotel-name | university arms hotel
Not matched: ../datasets/data/train/dialogues_134.json | PMUL0208.json | hotel-name | worth house
Not matched: ../datasets/data/train/dialogues_136.json | SSNG0348.json | hotel-name | archway house
Not matched: ../datasets/data/train/dialogues_137.json | MUL0120.json | taxi-destination | efes restaurant
Not matched: ../datasets/data/dev/dialogues_017.json | SNG01172.json | restaurant-name | efes restaurant
06/13/2021 05:10:19 - INFO - __main__ -    Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Use FP16 precision: False

loading configuration file https://huggingface.co/xlnet-large-cased/resolve/main/config.json from cache at /dhome/boewoei0123/.cache/huggingface/transformers/1f0d5fc4143aa8fe332810bac98d442fed5483549adcd9656e5709cd470003a0.a0945cddd1ef8f9d9c40c35c36bad4908625533057baeeafc6d26a9550f18c60
Model config XLNetConfig {
  "architectures": [
    "XLNetLMHeadModel"
  ],
  "attn_type": "bi",
  "bi_data": false,
  "bos_token_id": 1,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 4096,
  "d_model": 1024,
  "dropout": 0.1,
  "end_n_top": 5,
  "eos_token_id": 2,
  "ff_activation": "gelu",
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "model_type": "xlnet",
  "n_head": 16,
  "n_layer": 24,
  "pad_token_id": 5,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 250
    }
  },
  "transformers_version": "4.6.0",
  "untie_r": true,
  "use_mems_eval": true,
  "use_mems_train": false,
  "vocab_size": 32000
}

loading file https://huggingface.co/xlnet-large-cased/resolve/main/spiece.model from cache at /dhome/boewoei0123/.cache/huggingface/transformers/3af982b422f8bb8c510fdd1112afe6f5ec3f3219ef859edcf4c3826bec14832e.d93497120e3a865e2970f26abdf7bf375896f97fde8b874b70909592a6c785c9
loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer.json from cache at /dhome/boewoei0123/.cache/huggingface/transformers/6a4afd4829edeea0c7fe7735eccea233e66e79729e574966cfd9ec47f81d269a.2a683f915238b4f560dab0c724066cf0a7de9a851e96b0fb3a1e7f0881552f53
loading file https://huggingface.co/xlnet-large-cased/resolve/main/added_tokens.json from cache at None
loading file https://huggingface.co/xlnet-large-cased/resolve/main/special_tokens_map.json from cache at None
loading file https://huggingface.co/xlnet-large-cased/resolve/main/tokenizer_config.json from cache at None
06/13/2021 05:10:24 - INFO - __main__ -    Saving tokenizer to ./saved/0613-0510/tokenizer...
tokenizer config file saved in ./saved/0613-0510/tokenizer_config.json
Special tokens file saved in ./saved/0613-0510/special_tokens_map.json
loading weights file https://huggingface.co/xlnet-large-cased/resolve/main/pytorch_model.bin from cache at /dhome/boewoei0123/.cache/huggingface/transformers/0c2b00a768ca7c5b3534b75606a47a7e1125b10ce354b217022de5a12029859c.7fff7afe180c24f31dabdb196f95ca2e26a8aa357c1db6137f4fec6430db9776
Some weights of the model checkpoint at xlnet-large-cased were not used when initializing XLNetForQuestionAnswering: ['lm_loss.bias', 'lm_loss.weight']
- This IS expected if you are initializing XLNetForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing XLNetForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of XLNetForQuestionAnswering were not initialized from the model checkpoint at xlnet-large-cased and are newly initialized: ['end_logits.LayerNorm.bias', 'end_logits.dense_0.weight', 'end_logits.dense_1.bias', 'end_logits.dense_1.weight', 'answer_class.dense_0.bias', 'end_logits.dense_0.bias', 'answer_class.dense_0.weight', 'start_logits.dense.bias', 'answer_class.dense_1.weight', 'start_logits.dense.weight', 'end_logits.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
06/13/2021 05:10:29 - WARNING - datasets.builder -    Using custom data configuration default-05b2448cf3ade227
0 tables [00:00, ? tables/s]1 tables [00:00,  2.39 tables/s]                                0 tables [00:00, ? tables/s]                            Downloading and preparing dataset json/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /dhome/boewoei0123/.cache/huggingface/datasets/json/default-05b2448cf3ade227/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02...
Dataset json downloaded and prepared to /dhome/boewoei0123/.cache/huggingface/datasets/json/default-05b2448cf3ade227/0.0.0/83d5b3a2f62630efc6b5315f00f20209b4ad91a00ac586597caee3a4da0bef02. Subsequent calls will reuse this data.
 #0:   0%|          | 0/44 [00:00<?, ?ba/s]
 #1:   0%|          | 0/44 [00:00<?, ?ba/s][A

 #2:   0%|          | 0/44 [00:00<?, ?ba/s][A[A


 #3:   0%|          | 0/44 [00:00<?, ?ba/s][A[A[A
 #1:   2%|▏         | 1/44 [00:00<00:35,  1.20ba/s][A #0:   2%|▏         | 1/44 [00:00<00:42,  1.02ba/s]

 #2:   2%|▏         | 1/44 [00:00<00:40,  1.05ba/s][A[A


 #3:   2%|▏         | 1/44 [00:00<00:42,  1.01ba/s][A[A[A
 #1:   5%|▍         | 2/44 [00:01<00:36,  1.14ba/s][A


 #3:   5%|▍         | 2/44 [00:02<00:42,  1.01s/ba][A[A[A #0:   5%|▍         | 2/44 [00:02<00:42,  1.02s/ba]

 #2:   5%|▍         | 2/44 [00:02<00:42,  1.02s/ba][A[A
 #1:   7%|▋         | 3/44 [00:02<00:37,  1.09ba/s][A


 #3:   7%|▋         | 3/44 [00:03<00:41,  1.00s/ba][A[A[A #0:   7%|▋         | 3/44 [00:03<00:41,  1.02s/ba]

 #2:   7%|▋         | 3/44 [00:03<00:42,  1.03s/ba][A[A
 #1:   9%|▉         | 4/44 [00:03<00:37,  1.08ba/s][A #0:   9%|▉         | 4/44 [00:03<00:38,  1.04ba/s]


 #3:   9%|▉         | 4/44 [00:04<00:41,  1.03s/ba][A[A[A

 #2:   9%|▉         | 4/44 [00:04<00:43,  1.09s/ba][A[A #0:  11%|█▏        | 5/44 [00:04<00:36,  1.07ba/s]
 #1:  11%|█▏        | 5/44 [00:04<00:38,  1.01ba/s][A


 #3:  11%|█▏        | 5/44 [00:05<00:39,  1.01s/ba][A[A[A

 #2:  11%|█▏        | 5/44 [00:05<00:42,  1.09s/ba][A[A #0:  14%|█▎        | 6/44 [00:05<00:37,  1.00ba/s]
 #1:  14%|█▎        | 6/44 [00:06<00:39,  1.05s/ba][A


 #3:  14%|█▎        | 6/44 [00:06<00:39,  1.03s/ba][A[A[A

 #2:  14%|█▎        | 6/44 [00:06<00:40,  1.07s/ba][A[A #0:  16%|█▌        | 7/44 [00:07<00:38,  1.05s/ba]


 #3:  16%|█▌        | 7/44 [00:07<00:37,  1.03s/ba][A[A[A

 #2:  16%|█▌        | 7/44 [00:07<00:38,  1.04s/ba][A[A
 #1:  16%|█▌        | 7/44 [00:07<00:43,  1.19s/ba][A #0:  18%|█▊        | 8/44 [00:08<00:36,  1.03s/ba]


 #3:  18%|█▊        | 8/44 [00:08<00:37,  1.05s/ba][A[A[A

 #2:  18%|█▊        | 8/44 [00:08<00:37,  1.04s/ba][A[A #0:  20%|██        | 9/44 [00:09<00:36,  1.03s/ba]
 #1:  18%|█▊        | 8/44 [00:09<00:48,  1.35s/ba][A


 #3:  20%|██        | 9/44 [00:09<00:36,  1.05s/ba][A[A[A

 #2:  20%|██        | 9/44 [00:09<00:37,  1.08s/ba][A[A #0:  23%|██▎       | 10/44 [00:10<00:36,  1.06s/ba]


 #3:  23%|██▎       | 10/44 [00:10<00:35,  1.04s/ba][A[A[A
 #1:  20%|██        | 9/44 [00:10<00:47,  1.37s/ba][A

 #2:  23%|██▎       | 10/44 [00:10<00:38,  1.14s/ba][A[A


 #3:  25%|██▌       | 11/44 [00:11<00:33,  1.01s/ba][A[A[A #0:  25%|██▌       | 11/44 [00:11<00:36,  1.10s/ba]
 #1:  23%|██▎       | 10/44 [00:11<00:43,  1.27s/ba][A

 #2:  25%|██▌       | 11/44 [00:11<00:35,  1.08s/ba][A[A


 #3:  27%|██▋       | 12/44 [00:12<00:32,  1.00s/ba][A[A[A #0:  27%|██▋       | 12/44 [00:12<00:34,  1.07s/ba]

 #2:  27%|██▋       | 12/44 [00:12<00:32,  1.00s/ba][A[A
 #1:  25%|██▌       | 11/44 [00:12<00:39,  1.21s/ba][A


 #3:  30%|██▉       | 13/44 [00:13<00:31,  1.03s/ba][A[A[A #0:  30%|██▉       | 13/44 [00:13<00:33,  1.08s/ba]
 #1:  27%|██▋       | 12/44 [00:13<00:36,  1.13s/ba][A

 #2:  30%|██▉       | 13/44 [00:13<00:32,  1.06s/ba][A[A


 #3:  32%|███▏      | 14/44 [00:14<00:30,  1.01s/ba][A[A[A #0:  32%|███▏      | 14/44 [00:14<00:32,  1.09s/ba]
 #1:  30%|██▉       | 13/44 [00:14<00:34,  1.12s/ba][A

 #2:  32%|███▏      | 14/44 [00:15<00:34,  1.13s/ba][A[A


 #3:  34%|███▍      | 15/44 [00:15<00:29,  1.02s/ba][A[A[A #0:  34%|███▍      | 15/44 [00:15<00:31,  1.09s/ba]
 #1:  32%|███▏      | 14/44 [00:15<00:32,  1.07s/ba][A


 #3:  36%|███▋      | 16/44 [00:16<00:28,  1.00s/ba][A[A[A

 #2:  34%|███▍      | 15/44 [00:16<00:33,  1.16s/ba][A[A
 #1:  34%|███▍      | 15/44 [00:16<00:30,  1.07s/ba][A #0:  36%|███▋      | 16/44 [00:16<00:31,  1.12s/ba]


 #3:  39%|███▊      | 17/44 [00:17<00:27,  1.02s/ba][A[A[A

 #2:  36%|███▋      | 16/44 [00:17<00:32,  1.15s/ba][A[A
 #1:  36%|███▋      | 16/44 [00:18<00:30,  1.09s/ba][A #0:  39%|███▊      | 17/44 [00:18<00:31,  1.15s/ba]


 #3:  41%|████      | 18/44 [00:18<00:26,  1.02s/ba][A[A[A

 #2:  39%|███▊      | 17/44 [00:18<00:29,  1.11s/ba][A[A #0:  41%|████      | 18/44 [00:19<00:30,  1.16s/ba]
 #1:  39%|███▊      | 17/44 [00:19<00:31,  1.15s/ba][A


 #3:  43%|████▎     | 19/44 [00:19<00:24,  1.00ba/s][A[A[A

 #2:  41%|████      | 18/44 [00:19<00:28,  1.09s/ba][A[A #0:  43%|████▎     | 19/44 [00:20<00:27,  1.10s/ba]


 #3:  45%|████▌     | 20/44 [00:20<00:23,  1.00ba/s][A[A[A

 #2:  43%|████▎     | 19/44 [00:20<00:26,  1.06s/ba][A[A
 #1:  41%|████      | 18/44 [00:20<00:31,  1.23s/ba][A #0:  45%|████▌     | 20/44 [00:21<00:26,  1.10s/ba]


 #3:  48%|████▊     | 21/44 [00:21<00:23,  1.01s/ba][A[A[A

 #2:  45%|████▌     | 20/44 [00:21<00:23,  1.02ba/s][A[A
 #1:  43%|████▎     | 19/44 [00:21<00:28,  1.13s/ba][A

 #2:  48%|████▊     | 21/44 [00:22<00:22,  1.02ba/s][A[A


 #3:  50%|█████     | 22/44 [00:22<00:22,  1.02s/ba][A[A[A #0:  48%|████▊     | 21/44 [00:22<00:26,  1.15s/ba]
 #1:  45%|████▌     | 20/44 [00:22<00:26,  1.11s/ba][A


 #3:  52%|█████▏    | 23/44 [00:23<00:21,  1.01s/ba][A[A[A

 #2:  50%|█████     | 22/44 [00:23<00:22,  1.01s/ba][A[A #0:  50%|█████     | 22/44 [00:23<00:24,  1.12s/ba]
 #1:  48%|████▊     | 21/44 [00:23<00:26,  1.14s/ba][A


 #3:  55%|█████▍    | 24/44 [00:24<00:20,  1.03s/ba][A[A[A

 #2:  52%|█████▏    | 23/44 [00:24<00:21,  1.02s/ba][A[A #0:  52%|█████▏    | 23/44 [00:24<00:22,  1.06s/ba]
 #1:  50%|█████     | 22/44 [00:24<00:23,  1.08s/ba][A

 #2:  55%|█████▍    | 24/44 [00:25<00:19,  1.01ba/s][A[A


 #3:  57%|█████▋    | 25/44 [00:25<00:19,  1.02s/ba][A[A[A #0:  55%|█████▍    | 24/44 [00:26<00:23,  1.19s/ba]
 #1:  52%|█████▏    | 23/44 [00:26<00:24,  1.19s/ba][A


 #3:  59%|█████▉    | 26/44 [00:26<00:18,  1.03s/ba][A[A[A

 #2:  57%|█████▋    | 25/44 [00:26<00:21,  1.15s/ba][A[A #0:  57%|█████▋    | 25/44 [00:27<00:21,  1.15s/ba]
 #1:  55%|█████▍    | 24/44 [00:27<00:23,  1.18s/ba][A


 #3:  61%|██████▏   | 27/44 [00:27<00:17,  1.02s/ba][A[A[A #0:  59%|█████▉    | 26/44 [00:28<00:20,  1.12s/ba]

 #2:  59%|█████▉    | 26/44 [00:28<00:21,  1.18s/ba][A[A


 #3:  64%|██████▎   | 28/44 [00:28<00:16,  1.03s/ba][A[A[A
 #1:  57%|█████▋    | 25/44 [00:28<00:22,  1.19s/ba][A #0:  61%|██████▏   | 27/44 [00:29<00:18,  1.08s/ba]

 #2:  61%|██████▏   | 27/44 [00:29<00:19,  1.13s/ba][A[A
 #1:  59%|█████▉    | 26/44 [00:29<00:20,  1.14s/ba][A


 #3:  66%|██████▌   | 29/44 [00:29<00:15,  1.04s/ba][A[A[A

 #2:  64%|██████▎   | 28/44 [00:30<00:17,  1.07s/ba][A[A #0:  64%|██████▎   | 28/44 [00:30<00:17,  1.08s/ba]
 #1:  61%|██████▏   | 27/44 [00:30<00:17,  1.04s/ba][A


 #3:  68%|██████▊   | 30/44 [00:30<00:14,  1.02s/ba][A[A[A

 #2:  66%|██████▌   | 29/44 [00:31<00:15,  1.03s/ba][A[A
 #1:  64%|██████▎   | 28/44 [00:31<00:16,  1.01s/ba][A #0:  66%|██████▌   | 29/44 [00:31<00:16,  1.13s/ba]


 #3:  70%|███████   | 31/44 [00:31<00:13,  1.03s/ba][A[A[A

 #2:  68%|██████▊   | 30/44 [00:32<00:14,  1.04s/ba][A[A
 #1:  66%|██████▌   | 29/44 [00:32<00:14,  1.01ba/s][A


 #3:  73%|███████▎  | 32/44 [00:32<00:12,  1.00s/ba][A[A[A

 #2:  70%|███████   | 31/44 [00:33<00:13,  1.03s/ba][A[A #0:  68%|██████▊   | 30/44 [00:33<00:18,  1.32s/ba]


 #3:  75%|███████▌  | 33/44 [00:33<00:11,  1.01s/ba][A[A[A
 #1:  68%|██████▊   | 30/44 [00:33<00:15,  1.11s/ba][A

 #2:  73%|███████▎  | 32/44 [00:34<00:12,  1.07s/ba][A[A #0:  70%|███████   | 31/44 [00:34<00:16,  1.25s/ba]


 #3:  77%|███████▋  | 34/44 [00:34<00:09,  1.02ba/s][A[A[A

 #2:  75%|███████▌  | 33/44 [00:35<00:11,  1.04s/ba][A[A #0:  73%|███████▎  | 32/44 [00:35<00:14,  1.17s/ba]
 #1:  70%|███████   | 31/44 [00:35<00:16,  1.27s/ba][A


 #3:  80%|███████▉  | 35/44 [00:35<00:08,  1.03ba/s][A[A[A

 #2:  77%|███████▋  | 34/44 [00:36<00:09,  1.02ba/s][A[A


 #3:  82%|████████▏ | 36/44 [00:36<00:07,  1.01ba/s][A[A[A #0:  75%|███████▌  | 33/44 [00:36<00:13,  1.20s/ba]

 #2:  80%|███████▉  | 35/44 [00:37<00:08,  1.05ba/s][A[A
 #1:  73%|███████▎  | 32/44 [00:37<00:16,  1.39s/ba][A


 #3:  84%|████████▍ | 37/44 [00:37<00:07,  1.00s/ba][A[A[A #0:  77%|███████▋  | 34/44 [00:37<00:11,  1.15s/ba]

 #2:  82%|████████▏ | 36/44 [00:38<00:07,  1.01ba/s][A[A #0:  80%|███████▉  | 35/44 [00:38<00:09,  1.08s/ba]


 #3:  86%|████████▋ | 38/44 [00:38<00:05,  1.01ba/s][A[A[A
 #1:  75%|███████▌  | 33/44 [00:38<00:16,  1.52s/ba][A

 #2:  84%|████████▍ | 37/44 [00:39<00:07,  1.06s/ba][A[A


 #3:  89%|████████▊ | 39/44 [00:39<00:05,  1.03s/ba][A[A[A #0:  82%|████████▏ | 36/44 [00:39<00:09,  1.13s/ba]
 #1:  77%|███████▋  | 34/44 [00:40<00:14,  1.42s/ba][A

 #2:  86%|████████▋ | 38/44 [00:40<00:06,  1.07s/ba][A[A


 #3:  91%|█████████ | 40/44 [00:40<00:04,  1.01s/ba][A[A[A #0:  84%|████████▍ | 37/44 [00:40<00:07,  1.11s/ba]
 #1:  80%|███████▉  | 35/44 [00:41<00:11,  1.32s/ba][A

 #2:  89%|████████▊ | 39/44 [00:41<00:05,  1.08s/ba][A[A


 #3:  93%|█████████▎| 41/44 [00:41<00:03,  1.02s/ba][A[A[A #0:  86%|████████▋ | 38/44 [00:41<00:06,  1.09s/ba]

 #2:  91%|█████████ | 40/44 [00:42<00:04,  1.08s/ba][A[A


 #3:  95%|█████████▌| 42/44 [00:42<00:02,  1.02s/ba][A[A[A
 #1:  82%|████████▏ | 36/44 [00:42<00:11,  1.40s/ba][A #0:  89%|████████▊ | 39/44 [00:43<00:06,  1.25s/ba]

 #2:  93%|█████████▎| 41/44 [00:43<00:03,  1.08s/ba][A[A


 #3:  98%|█████████▊| 43/44 [00:43<00:01,  1.02s/ba][A[A[A
 #1:  84%|████████▍ | 37/44 [00:44<00:09,  1.43s/ba][A


 #3: 100%|██████████| 44/44 [00:44<00:00,  1.07ba/s][A[A[A #3: 100%|██████████| 44/44 [00:44<00:00,  1.01s/ba]

 #2:  95%|█████████▌| 42/44 [00:44<00:02,  1.06s/ba][A[A #0:  91%|█████████ | 40/44 [00:45<00:05,  1.41s/ba]
 #1:  86%|████████▋ | 38/44 [00:45<00:08,  1.36s/ba][A

 #2:  98%|█████████▊| 43/44 [00:45<00:01,  1.07s/ba][A[A

 #2: 100%|██████████| 44/44 [00:46<00:00,  1.01ba/s][A[A #0:  93%|█████████▎| 41/44 [00:46<00:04,  1.38s/ba] #2: 100%|██████████| 44/44 [00:46<00:00,  1.06s/ba]
 #1:  89%|████████▊ | 39/44 [00:47<00:07,  1.42s/ba][A #0:  95%|█████████▌| 42/44 [00:47<00:02,  1.31s/ba]
 #1:  91%|█████████ | 40/44 [00:48<00:05,  1.43s/ba][A #0:  98%|█████████▊| 43/44 [00:48<00:01,  1.23s/ba]
 #1:  93%|█████████▎| 41/44 [00:49<00:03,  1.31s/ba][A #0: 100%|██████████| 44/44 [00:49<00:00,  1.07s/ba] #0: 100%|██████████| 44/44 [00:49<00:00,  1.13s/ba]
 #1:  95%|█████████▌| 42/44 [00:50<00:02,  1.19s/ba][A
 #1:  98%|█████████▊| 43/44 [00:51<00:01,  1.07s/ba][A
 #1: 100%|██████████| 44/44 [00:51<00:00,  1.06ba/s][A #1: 100%|██████████| 44/44 [00:51<00:00,  1.18s/ba]



 #0:   0%|          | 0/7 [00:00<?, ?ba/s]
 #1:   0%|          | 0/7 [00:00<?, ?ba/s][A

 #2:   0%|          | 0/7 [00:00<?, ?ba/s][A[A


 #3:   0%|          | 0/7 [00:00<?, ?ba/s][A[A[A

 #2:  14%|█▍        | 1/7 [00:01<00:11,  1.89s/ba][A[A #0:  14%|█▍        | 1/7 [00:01<00:11,  1.96s/ba]


 #3:  14%|█▍        | 1/7 [00:02<00:13,  2.24s/ba][A[A[A
 #1:  14%|█▍        | 1/7 [00:02<00:17,  2.95s/ba][A #0:  29%|██▊       | 2/7 [00:04<00:10,  2.01s/ba]

 #2:  29%|██▊       | 2/7 [00:04<00:10,  2.04s/ba][A[A


 #3:  29%|██▊       | 2/7 [00:04<00:11,  2.21s/ba][A[A[A
 #1:  29%|██▊       | 2/7 [00:04<00:13,  2.60s/ba][A #0:  43%|████▎     | 3/7 [00:06<00:07,  1.98s/ba]

 #2:  43%|████▎     | 3/7 [00:06<00:07,  1.99s/ba][A[A


 #3:  43%|████▎     | 3/7 [00:06<00:08,  2.21s/ba][A[A[A
 #1:  43%|████▎     | 3/7 [00:06<00:09,  2.49s/ba][A #0:  57%|█████▋    | 4/7 [00:07<00:05,  1.98s/ba]

 #2:  57%|█████▋    | 4/7 [00:08<00:06,  2.13s/ba][A[A


 #3:  57%|█████▋    | 4/7 [00:08<00:06,  2.17s/ba][A[A[A
 #1:  57%|█████▋    | 4/7 [00:09<00:07,  2.39s/ba][A


 #3:  71%|███████▏  | 5/7 [00:10<00:04,  2.15s/ba][A[A[A #0:  71%|███████▏  | 5/7 [00:10<00:04,  2.28s/ba]

 #2:  71%|███████▏  | 5/7 [00:11<00:04,  2.22s/ba][A[A
 #1:  71%|███████▏  | 5/7 [00:11<00:04,  2.31s/ba][A


 #3:  86%|████████▌ | 6/7 [00:12<00:02,  2.13s/ba][A[A[A

 #2:  86%|████████▌ | 6/7 [00:13<00:02,  2.21s/ba][A[A #0:  86%|████████▌ | 6/7 [00:13<00:02,  2.30s/ba]


 #3: 100%|██████████| 7/7 [00:13<00:00,  1.66s/ba][A[A[A #3: 100%|██████████| 7/7 [00:13<00:00,  1.92s/ba]

 #2: 100%|██████████| 7/7 [00:13<00:00,  1.70s/ba][A[A #2: 100%|██████████| 7/7 [00:13<00:00,  1.96s/ba] #0: 100%|██████████| 7/7 [00:13<00:00,  1.79s/ba] #0: 100%|██████████| 7/7 [00:13<00:00,  1.99s/ba]
 #1:  86%|████████▌ | 6/7 [00:14<00:02,  2.51s/ba][A
 #1: 100%|██████████| 7/7 [00:14<00:00,  1.91s/ba][A #1: 100%|██████████| 7/7 [00:14<00:00,  2.11s/ba]


06/13/2021 05:11:41 - INFO - __main__ -    
******** Running training ********
06/13/2021 05:11:41 - INFO - __main__ -    Num train examples = 188211
06/13/2021 05:11:41 - INFO - __main__ -    Num Epochs = 4
06/13/2021 05:11:41 - INFO - __main__ -    Instantaneous batch size per device = 4
06/13/2021 05:11:41 - INFO - __main__ -    Total train batch size (w/ parallel, distributed & accumulation) = 64
06/13/2021 05:11:41 - INFO - __main__ -    Instantaneous steps per epoch = 47053
06/13/2021 05:11:41 - INFO - __main__ -    Update steps per epoch = 2941
06/13/2021 05:11:41 - INFO - __main__ -    Total update steps = 11764
06/13/2021 05:11:41 - INFO - __main__ -    
Epoch 01 / 04
06/13/2021 05:19:23 - INFO - __main__ -    Train | Loss: 6.79744
06/13/2021 05:27:10 - INFO - __main__ -    Train | Loss: 5.60674
06/13/2021 05:34:56 - INFO - __main__ -    Train | Loss: 4.59260
06/13/2021 05:42:41 - INFO - __main__ -    Train | Loss: 3.87477
06/13/2021 05:50:25 - INFO - __main__ -    Train | Loss: 3.26697
06/13/2021 05:58:12 - INFO - __main__ -    Train | Loss: 2.81120
06/13/2021 06:05:51 - INFO - __main__ -    Train | Loss: 2.46550
06/13/2021 06:13:36 - INFO - __main__ -    Train | Loss: 2.20184
06/13/2021 06:21:24 - INFO - __main__ -    Train | Loss: 1.99002
06/13/2021 06:29:08 - INFO - __main__ -    Train | Loss: 1.81497
06/13/2021 06:36:54 - INFO - __main__ -    Train | Loss: 1.66897
06/13/2021 06:44:42 - INFO - __main__ -    Train | Loss: 1.54840
06/13/2021 06:52:30 - INFO - __main__ -    Train | Loss: 1.44482
06/13/2021 07:00:17 - INFO - __main__ -    Train | Loss: 1.35454
06/13/2021 07:08:06 - INFO - __main__ -    Train | Loss: 1.27592
06/13/2021 07:15:56 - INFO - __main__ -    Train | Loss: 1.20618
06/13/2021 07:23:47 - INFO - __main__ -    Train | Loss: 1.14456
06/13/2021 07:31:35 - INFO - __main__ -    Train | Loss: 1.09005
06/13/2021 07:39:19 - INFO - __main__ -    Train | Loss: 1.04059
06/13/2021 07:46:59 - INFO - __main__ -    Train | Loss: 0.99440
06/13/2021 07:54:42 - INFO - __main__ -    Train | Loss: 0.95328
06/13/2021 08:02:23 - INFO - __main__ -    Train | Loss: 0.91635
06/13/2021 08:10:05 - INFO - __main__ -    Train | Loss: 0.88155
06/13/2021 08:17:49 - INFO - __main__ -    Train | Loss: 0.84993
06/13/2021 08:25:33 - INFO - __main__ -    Train | Loss: 0.82044
06/13/2021 08:56:04 - INFO - pred_utils -    Post-processing 25094 example predictions split into 26861 features.
06/13/2021 08:57:16 - INFO - __main__ -    Valid | MAA: 0.97673, JAA: 0.82085, MEM: 0.91010, JEM: 0.71783, MGA: 0.94050, JGA: 0.60016
Configuration saved in ./saved/0613-0510/config.json
Model weights saved in ./saved/0613-0510/pytorch_model.bin
06/13/2021 08:57:17 - INFO - __main__ -    Saving config and model to ./saved/0613-0510...
06/13/2021 09:05:01 - INFO - __main__ -    Train | Loss: 0.79251
06/13/2021 09:12:41 - INFO - __main__ -    Train | Loss: 0.76666
06/13/2021 09:20:20 - INFO - __main__ -    Train | Loss: 0.74328
06/13/2021 09:27:58 - INFO - __main__ -    Train | Loss: 0.72115
06/13/2021 09:35:40 - INFO - __main__ -    Train | Loss: 0.69996
06/13/2021 09:43:20 - INFO - __main__ -    Train | Loss: 0.68049
06/13/2021 09:51:00 - INFO - __main__ -    Train | Loss: 0.66188
06/13/2021 09:58:39 - INFO - __main__ -    Train | Loss: 0.64427
06/13/2021 10:06:18 - INFO - __main__ -    Train | Loss: 0.62769
06/13/2021 10:14:01 - INFO - __main__ -    Train | Loss: 0.61225
06/13/2021 10:21:51 - INFO - __main__ -    Train | Loss: 0.59741
06/13/2021 10:29:40 - INFO - __main__ -    Train | Loss: 0.58373
06/13/2021 10:37:27 - INFO - __main__ -    Train | Loss: 0.57071
06/13/2021 10:45:12 - INFO - __main__ -    Train | Loss: 0.55788
06/13/2021 10:52:52 - INFO - __main__ -    Train | Loss: 0.54599
06/13/2021 11:00:34 - INFO - __main__ -    Train | Loss: 0.53474
06/13/2021 11:08:19 - INFO - __main__ -    Train | Loss: 0.52369
06/13/2021 11:16:05 - INFO - __main__ -    Train | Loss: 0.51335
06/13/2021 11:23:50 - INFO - __main__ -    Train | Loss: 0.50339
06/13/2021 11:31:36 - INFO - __main__ -    Train | Loss: 0.49371
06/13/2021 11:39:22 - INFO - __main__ -    Train | Loss: 0.48431
06/13/2021 11:47:03 - INFO - __main__ -    Train | Loss: 0.47541
06/13/2021 11:47:27 - INFO - __main__ -    Train | Loss: 0.47496
06/13/2021 12:17:50 - INFO - pred_utils -    Post-processing 25094 example predictions split into 26861 features.
06/13/2021 12:19:05 - INFO - __main__ -    Valid | MAA: 0.97545, JAA: 0.82166, MEM: 0.90457, JEM: 0.70033, MGA: 0.93879, JGA: 0.59324
06/13/2021 12:19:05 - INFO - __main__ -    
Epoch 02 / 04
06/13/2021 12:26:51 - INFO - __main__ -    Train | Loss: 0.06524
06/13/2021 12:34:33 - INFO - __main__ -    Train | Loss: 0.06489
06/13/2021 12:42:19 - INFO - __main__ -    Train | Loss: 0.06258
06/13/2021 12:49:57 - INFO - __main__ -    Train | Loss: 0.06216
06/13/2021 12:57:40 - INFO - __main__ -    Train | Loss: 0.06191
06/13/2021 13:05:21 - INFO - __main__ -    Train | Loss: 0.06024
06/13/2021 13:13:02 - INFO - __main__ -    Train | Loss: 0.06070
06/13/2021 13:20:42 - INFO - __main__ -    Train | Loss: 0.06054
06/13/2021 13:28:26 - INFO - __main__ -    Train | Loss: 0.06103
06/13/2021 13:36:08 - INFO - __main__ -    Train | Loss: 0.06086
06/13/2021 13:43:52 - INFO - __main__ -    Train | Loss: 0.06050
06/13/2021 13:51:36 - INFO - __main__ -    Train | Loss: 0.06013
06/13/2021 13:59:18 - INFO - __main__ -    Train | Loss: 0.06016
06/13/2021 14:07:03 - INFO - __main__ -    Train | Loss: 0.05971
06/13/2021 14:14:46 - INFO - __main__ -    Train | Loss: 0.05867
06/13/2021 14:22:32 - INFO - __main__ -    Train | Loss: 0.05846
06/13/2021 14:30:15 - INFO - __main__ -    Train | Loss: 0.05801
06/13/2021 14:38:01 - INFO - __main__ -    Train | Loss: 0.05800
06/13/2021 14:45:46 - INFO - __main__ -    Train | Loss: 0.05770
06/13/2021 14:53:30 - INFO - __main__ -    Train | Loss: 0.05764
06/13/2021 15:01:14 - INFO - __main__ -    Train | Loss: 0.05772
06/13/2021 15:08:54 - INFO - __main__ -    Train | Loss: 0.05739
06/13/2021 15:16:33 - INFO - __main__ -    Train | Loss: 0.05754
06/13/2021 15:24:13 - INFO - __main__ -    Train | Loss: 0.05729
06/13/2021 15:31:53 - INFO - __main__ -    Train | Loss: 0.05707
06/13/2021 16:02:26 - INFO - pred_utils -    Post-processing 25094 example predictions split into 26861 features.
06/13/2021 16:03:35 - INFO - __main__ -    Valid | MAA: 0.98278, JAA: 0.86604, MEM: 0.92087, JEM: 0.75081, MGA: 0.95218, JGA: 0.66979
Configuration saved in ./saved/0613-0510/config.json
Model weights saved in ./saved/0613-0510/pytorch_model.bin
06/13/2021 16:03:41 - INFO - __main__ -    Saving config and model to ./saved/0613-0510...
06/13/2021 16:11:24 - INFO - __main__ -    Train | Loss: 0.05697
06/13/2021 16:19:06 - INFO - __main__ -    Train | Loss: 0.05663
06/13/2021 16:26:47 - INFO - __main__ -    Train | Loss: 0.05620
06/13/2021 16:34:27 - INFO - __main__ -    Train | Loss: 0.05595
06/13/2021 16:42:07 - INFO - __main__ -    Train | Loss: 0.05575
06/13/2021 16:49:47 - INFO - __main__ -    Train | Loss: 0.05539
06/13/2021 16:57:26 - INFO - __main__ -    Train | Loss: 0.05522
06/13/2021 17:05:06 - INFO - __main__ -    Train | Loss: 0.05535
06/13/2021 17:12:44 - INFO - __main__ -    Train | Loss: 0.05515
06/13/2021 17:20:31 - INFO - __main__ -    Train | Loss: 0.05505
06/13/2021 17:28:09 - INFO - __main__ -    Train | Loss: 0.05479
06/13/2021 17:35:51 - INFO - __main__ -    Train | Loss: 0.05455
06/13/2021 17:43:29 - INFO - __main__ -    Train | Loss: 0.05436
06/13/2021 17:51:13 - INFO - __main__ -    Train | Loss: 0.05403
06/13/2021 17:58:52 - INFO - __main__ -    Train | Loss: 0.05373
06/13/2021 18:06:32 - INFO - __main__ -    Train | Loss: 0.05352
06/13/2021 18:14:10 - INFO - __main__ -    Train | Loss: 0.05329
06/13/2021 18:21:49 - INFO - __main__ -    Train | Loss: 0.05319
